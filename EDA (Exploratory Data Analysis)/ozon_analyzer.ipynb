{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install polars-lts-cpu\n",
    "# ДОБАВЬТЕ ЭТО В САМУЮ ПЕРВУЮ ЯЧЕЙКУ NOTEBOOK\n",
    "import os\n",
    "os.environ['POLARS_SKIP_CPU_CHECK'] = '1'\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d8ddf",
   "metadata": {},
   "source": [
    "## Этап 1: Детальное исследование данных (EDA) и проектирование фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd84b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119241</td>\n",
       "      <td>1156930</td>\n",
       "      <td>2025-07-03 08:30:48.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>350526</td>\n",
       "      <td>3772191</td>\n",
       "      <td>2025-07-03 12:51:19.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>427645</td>\n",
       "      <td>4953991</td>\n",
       "      <td>2025-07-03 04:09:18.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>617762</td>\n",
       "      <td>3048491</td>\n",
       "      <td>2025-07-03 16:15:38.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>936816</td>\n",
       "      <td>1079151</td>\n",
       "      <td>2025-07-03 15:21:25.660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  user_id       created_timestamp\n",
       "0   119241  1156930 2025-07-03 08:30:48.743\n",
       "1   350526  3772191 2025-07-03 12:51:19.050\n",
       "2   427645  4953991 2025-07-03 04:09:18.837\n",
       "3   617762  3048491 2025-07-03 16:15:38.360\n",
       "4   936816  1079151 2025-07-03 15:21:25.660"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_test_for_participants\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_test_for_participants/test_for_participants/created_date=2025-07-03/part-00000-3020be67-2ce5-4232-b8de-251f063b386a.c000.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c8da8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>itemname</th>\n",
       "      <th>attributes</th>\n",
       "      <th>fclip_embed</th>\n",
       "      <th>catalogid</th>\n",
       "      <th>variant_id</th>\n",
       "      <th>model_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400301</td>\n",
       "      <td>Очки солнцезащитные</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.32023853, 0.11766258, -0.784552, 0.4168199,...</td>\n",
       "      <td>17080</td>\n",
       "      <td>243954236</td>\n",
       "      <td>33306429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>678602</td>\n",
       "      <td>Футболка GOLDUSTIM Футболка женская</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.4360095, 0.6339668, 0.029741853, 0.09847632...</td>\n",
       "      <td>7508</td>\n",
       "      <td>8039633</td>\n",
       "      <td>27057398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>746401</td>\n",
       "      <td>Худи Patagonia</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.47452784, -0.4727797, -0.51971936, -0.08221...</td>\n",
       "      <td>7555</td>\n",
       "      <td>120078071</td>\n",
       "      <td>11645051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>872829</td>\n",
       "      <td>Комплект носков adidas 3S C Spw Mid 3P, 3 пары</td>\n",
       "      <td>None</td>\n",
       "      <td>[-0.06987186, -2.2802832, -0.5829071, -0.33674...</td>\n",
       "      <td>36563</td>\n",
       "      <td>247120069</td>\n",
       "      <td>38088455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>893255</td>\n",
       "      <td>Жилет Buy &amp; Style жилеты</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.12488763, 0.5466293, 0.29031003, 0.21499962...</td>\n",
       "      <td>7535</td>\n",
       "      <td>327822613</td>\n",
       "      <td>19339929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                        itemname attributes                                        fclip_embed  catalogid  variant_id  model_id\n",
       "0   400301                             Очки солнцезащитные       None  [0.32023853, 0.11766258, -0.784552, 0.4168199,...      17080   243954236  33306429\n",
       "1   678602             Футболка GOLDUSTIM Футболка женская       None  [0.4360095, 0.6339668, 0.029741853, 0.09847632...       7508     8039633  27057398\n",
       "2   746401                                  Худи Patagonia       None  [0.47452784, -0.4727797, -0.51971936, -0.08221...       7555   120078071  11645051\n",
       "3   872829  Комплект носков adidas 3S C Spw Mid 3P, 3 пары       None  [-0.06987186, -2.2802832, -0.5829071, -0.33674...      36563   247120069  38088455\n",
       "4   893255                        Жилет Buy & Style жилеты       None  [0.12488763, 0.5466293, 0.29031003, 0.21499962...       7535   327822613  19339929"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_train_final_apparel_items_data\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_train_final_apparel_items_data/ml_ozon_recsys_train_final_apparel_items_data/part-00000-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fd97f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_timestamp</th>\n",
       "      <th>last_status</th>\n",
       "      <th>last_status_timestamp</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15535</td>\n",
       "      <td>3423970</td>\n",
       "      <td>2025-01-11 19:20:02.550</td>\n",
       "      <td>delivered_orders</td>\n",
       "      <td>2025-01-16 10:53:03</td>\n",
       "      <td>2025-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15535</td>\n",
       "      <td>1207601</td>\n",
       "      <td>2025-03-25 13:11:48.240</td>\n",
       "      <td>canceled_orders</td>\n",
       "      <td>2025-03-29 08:32:43</td>\n",
       "      <td>2025-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119241</td>\n",
       "      <td>2340021</td>\n",
       "      <td>2025-05-03 20:46:18.146</td>\n",
       "      <td>canceled_orders</td>\n",
       "      <td>2025-05-12 17:10:27</td>\n",
       "      <td>2025-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119241</td>\n",
       "      <td>2356390</td>\n",
       "      <td>2025-05-01 17:53:00.716</td>\n",
       "      <td>delivered_orders</td>\n",
       "      <td>2025-05-14 14:36:20</td>\n",
       "      <td>2025-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119241</td>\n",
       "      <td>1490080</td>\n",
       "      <td>2025-04-22 22:59:11.160</td>\n",
       "      <td>canceled_orders</td>\n",
       "      <td>2025-04-26 06:27:28</td>\n",
       "      <td>2025-04-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  user_id       created_timestamp       last_status last_status_timestamp created_date\n",
       "0    15535  3423970 2025-01-11 19:20:02.550  delivered_orders   2025-01-16 10:53:03   2025-01-11\n",
       "1    15535  1207601 2025-03-25 13:11:48.240   canceled_orders   2025-03-29 08:32:43   2025-03-25\n",
       "2   119241  2340021 2025-05-03 20:46:18.146   canceled_orders   2025-05-12 17:10:27   2025-05-03\n",
       "3   119241  2356390 2025-05-01 17:53:00.716  delivered_orders   2025-05-14 14:36:20   2025-05-01\n",
       "4   119241  1490080 2025-04-22 22:59:11.160   canceled_orders   2025-04-26 06:27:28   2025-04-22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_train_final_apparel_orders_data\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_train_final_apparel_orders_data/ml_ozon_recsys_train_final_apparel_orders_data/part-00000-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c43d81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action_widget</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>678602</td>\n",
       "      <td>4211601</td>\n",
       "      <td>2025-05-05 18:11:15</td>\n",
       "      <td>page_view</td>\n",
       "      <td>cart.cartSplit</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>678602</td>\n",
       "      <td>4211601</td>\n",
       "      <td>2025-05-05 18:12:57</td>\n",
       "      <td>view_description</td>\n",
       "      <td>pdp.characteristics</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>678602</td>\n",
       "      <td>4211601</td>\n",
       "      <td>2025-05-05 18:12:52</td>\n",
       "      <td>view_description</td>\n",
       "      <td>pdp.richContent</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>678602</td>\n",
       "      <td>4211601</td>\n",
       "      <td>2025-05-05 10:39:08</td>\n",
       "      <td>page_view</td>\n",
       "      <td>cart.cartSplit</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>678602</td>\n",
       "      <td>4211601</td>\n",
       "      <td>2025-05-05 18:12:57</td>\n",
       "      <td>view_description</td>\n",
       "      <td>pdp.characteristics</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  user_id           timestamp       action_type        action_widget        date\n",
       "0   678602  4211601 2025-05-05 18:11:15         page_view       cart.cartSplit  2025-05-05\n",
       "1   678602  4211601 2025-05-05 18:12:57  view_description  pdp.characteristics  2025-05-05\n",
       "2   678602  4211601 2025-05-05 18:12:52  view_description      pdp.richContent  2025-05-05\n",
       "3   678602  4211601 2025-05-05 10:39:08         page_view       cart.cartSplit  2025-05-05\n",
       "4   678602  4211601 2025-05-05 18:12:57  view_description  pdp.characteristics  2025-05-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_train_final_apparel_tracker_data\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_train_final_apparel_tracker_data/ml_ozon_recsys_train_final_apparel_tracker_data/part-00000-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a93d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalogid</th>\n",
       "      <th>catalogpath</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15500</td>\n",
       "      <td>None</td>\n",
       "      <td>[15500, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15501</td>\n",
       "      <td>None</td>\n",
       "      <td>[15501, 15500, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15502</td>\n",
       "      <td>None</td>\n",
       "      <td>[15502, 15501, 15500, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15511</td>\n",
       "      <td>None</td>\n",
       "      <td>[15511, 15501, 15500, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15892</td>\n",
       "      <td>None</td>\n",
       "      <td>[15892, 15511, 15501, 15500, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   catalogid catalogpath                               ids\n",
       "0      15500        None                       [15500, -1]\n",
       "1      15501        None                [15501, 15500, -1]\n",
       "2      15502        None         [15502, 15501, 15500, -1]\n",
       "3      15511        None         [15511, 15501, 15500, -1]\n",
       "4      15892        None  [15892, 15511, 15501, 15500, -1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_train_final_categories_tree\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_train_final_categories_tree/ml_ozon_recsys_train_final_categories_tree/part-00000-163f9108-23d6-4fd6-98c4-7bcb73ee27e2-c000.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bfba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1857340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4815130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3530170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id\n",
       "0  1857340\n",
       "1  4094281\n",
       "2  4815130\n",
       "3    38621\n",
       "4  3530170"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтение файла ml_ozon_recsys_test.snappy\n",
    "df = pd.read_parquet('/home/dima/ozon/data/ml_ozon_recsys_test_for_participants/test_for_participants/ml_ozon_recsys_test.snappy.parquet', engine='fastparquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск полного анализа данных с рекурсивным поиском\n",
      "============================================================\n",
      "Загрузка данных с рекурсивным поиском...\n",
      "\n",
      "Поиск файлов в ml_ozon_recsys_test_for_participants...\n",
      "ml_ozon_recsys_test_for_participants: найдено 141 .parquet файлов\n",
      "   Найдено 71 валидных файлов (исключены служебные)\n",
      "   1. ml_ozon_recsys_test.snappy.parquet (1.8 MB)\n",
      "   2. part-00003-3020be67-2ce5-4232-b8de-251f063b386a.c000.snappy.parquet (0.2 MB)\n",
      "   3. part-00000-3020be67-2ce5-4232-b8de-251f063b386a.c000.snappy.parquet (0.2 MB)\n",
      "   ... и еще 68 файлов\n",
      "Чтение первых 2 файлов из 71...\n",
      "   Чтение: ml_ozon_recsys_test.snappy.parquet (1.8 MB)\n",
      "   Схема: ['user_id']\n",
      "  Успешно: 5000 строк, 1 столбцов\n",
      "  Используем схему: ['user_id']\n",
      "   Чтение: part-00003-3020be67-2ce5-4232-b8de-251f063b386a.c000.snappy.parquet (0.2 MB)\n",
      "   Схема: ['item_id', 'user_id', 'created_timestamp']\n",
      "  Успешно: 5000 строк, 3 столбцов\n",
      "  Пропускаем файл part-00003-3020be67-2ce5-4232-b8de-251f063b386a.c000.snappy.parquet - несовместимая схема\n",
      "  Ожидалось: ['user_id']\n",
      "  Получено: ['item_id', 'user_id', 'created_timestamp']\n",
      "✓ ml_ozon_recsys_test_for_participants: загружено 5000 строк из 2 файлов\n",
      "\n",
      "Поиск файлов в ml_ozon_recsys_train_final_categories_tree...\n",
      "ml_ozon_recsys_train_final_categories_tree: найдено 2 .parquet файлов\n",
      "   Найдено 1 валидных файлов (исключены служебные)\n",
      "   1. part-00000-163f9108-23d6-4fd6-98c4-7bcb73ee27e2-c000.snappy.parquet (0.5 MB)\n",
      "Чтение первых 1 файлов из 1...\n",
      "   Чтение: part-00000-163f9108-23d6-4fd6-98c4-7bcb73ee27e2-c000.snappy.parquet (0.5 MB)\n",
      "   Схема: ['catalogid', 'catalogpath', 'ids']\n",
      "  Успешно: 7012 строк, 3 столбцов\n",
      "  Используем схему: ['catalogid', 'catalogpath', 'ids']\n",
      "✓ ml_ozon_recsys_train_final_categories_tree: загружено 7012 строк из 1 файлов\n",
      "\n",
      "Поиск файлов в ml_ozon_recsys_train_final_apparel_items_data...\n",
      "ml_ozon_recsys_train_final_apparel_items_data: найдено 200 .parquet файлов\n",
      "   Найдено 100 валидных файлов (исключены служебные)\n",
      "   1. part-00030-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet (188.8 MB)\n",
      "   2. part-00078-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet (188.8 MB)\n",
      "   3. part-00089-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet (187.8 MB)\n",
      "   ... и еще 97 файлов\n",
      "Чтение первых 2 файлов из 100...\n",
      "   Чтение: part-00030-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet (188.8 MB)\n",
      "   Схема: ['item_id', 'itemname', 'attributes', 'fclip_embed', 'catalogid', 'variant_id', 'model_id']\n",
      "  Успешно: 3000 строк, 5 столбцов\n",
      "  Используем схему: ['item_id', 'itemname', 'catalogid', 'variant_id', 'model_id']\n",
      "   Чтение: part-00078-5322ad28-d73d-4c69-86d2-4efc7ad4f422-c000.snappy.parquet (188.8 MB)\n",
      "   Схема: ['item_id', 'itemname', 'attributes', 'fclip_embed', 'catalogid', 'variant_id', 'model_id']\n",
      "  Успешно: 3000 строк, 5 столбцов\n",
      "✓ ml_ozon_recsys_train_final_apparel_items_data: загружено 6000 строк из 2 файлов\n",
      "\n",
      "Поиск файлов в ml_ozon_recsys_train_final_apparel_orders_data...\n",
      "ml_ozon_recsys_train_final_apparel_orders_data: найдено 10 .parquet файлов\n",
      "   Найдено 5 валидных файлов (исключены служебные)\n",
      "   1. part-00004-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet (89.7 MB)\n",
      "   2. part-00003-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet (90.4 MB)\n",
      "   3. part-00000-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet (89.9 MB)\n",
      "   ... и еще 2 файлов\n",
      "Чтение первых 2 файлов из 5...\n",
      "   Чтение: part-00004-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet (89.7 MB)\n",
      "   Схема: ['item_id', 'user_id', 'created_timestamp', 'last_status', 'last_status_timestamp', 'created_date']\n",
      "  Успешно: 2000 строк, 4 столбцов\n",
      "  Используем схему: ['item_id', 'user_id', 'last_status', 'created_timestamp']\n",
      "   Чтение: part-00003-7b718cf5-560c-4ace-bbb7-e36ef433637b-c000.snappy.parquet (90.4 MB)\n",
      "   Схема: ['item_id', 'user_id', 'created_timestamp', 'last_status', 'last_status_timestamp', 'created_date']\n",
      "  Успешно: 2000 строк, 4 столбцов\n",
      "✓ ml_ozon_recsys_train_final_apparel_orders_data: загружено 4000 строк из 2 файлов\n",
      "\n",
      "Поиск файлов в ml_ozon_recsys_train_final_apparel_tracker_data...\n",
      "ml_ozon_recsys_train_final_apparel_tracker_data: найдено 400 .parquet файлов\n",
      "   Найдено 200 валидных файлов (исключены служебные)\n",
      "   1. part-00020-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet (97.0 MB)\n",
      "   2. part-00025-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet (102.1 MB)\n",
      "   3. part-00034-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet (98.8 MB)\n",
      "   ... и еще 197 файлов\n",
      "Чтение первых 2 файлов из 200...\n",
      "   Чтение: part-00020-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet (97.0 MB)\n",
      "   Схема: ['item_id', 'user_id', 'timestamp', 'action_type', 'action_widget', 'date']\n",
      "  Успешно: 1000 строк, 4 столбцов\n",
      "  Используем схему: ['item_id', 'user_id', 'timestamp', 'action_type']\n",
      "   Чтение: part-00025-582d88ee-8dd6-41f6-9f90-375910468cb7-c000.snappy.parquet (102.1 MB)\n",
      "   Схема: ['item_id', 'user_id', 'timestamp', 'action_type', 'action_widget', 'date']\n",
      "  Успешно: 1000 строк, 4 столбцов\n",
      "✓ ml_ozon_recsys_train_final_apparel_tracker_data: загружено 2000 строк из 2 файлов\n",
      "Анализ тестовых данных.\n",
      "  Столбец 'item_id' не найден в тестовых данных\n",
      "  Столбец 'created_timestamp' не найден в тестовых данных\n",
      "Анализ данных товаров.\n",
      "Анализ данных заказов.\n",
      "Анализ данных трекера.\n",
      "Анализ дерева категорий.\n",
      "Анализ тестовых данных.\n",
      "  Столбец 'item_id' не найден в тестовых данных\n",
      "  Столбец 'created_timestamp' не найден в тестовых данных\n",
      "Создана визуализация статусов заказов\n",
      "Создана визуализация действий трекера\n",
      "Создано 2 визуализаций\n",
      "Анализ успешно завершен!\n",
      "Результаты сохранены в eda_results.json\n",
      "\n",
      "Сводная статистика:\n",
      "Проанализировано датасетов: 5\n",
      "Общее количество строк: 24012\n",
      "Пиковое использование памяти: 2449.6 MB\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "class OzonDataAnalyzer:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.dfs = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def filter_valid_parquet_files(self, file_paths):\n",
    "        \"\"\"Фильтруем только настоящие parquet файлы (исключаем служебные)\"\"\"\n",
    "        valid_files = []\n",
    "        for file_path in file_paths:\n",
    "            # Исключаем файлы macOS и слишком маленькие файлы\n",
    "            if not file_path.name.startswith('._') and file_path.stat().st_size > 1024:  # > 1KB\n",
    "                valid_files.append(file_path)\n",
    "        \n",
    "        print(f\"   Найдено {len(valid_files)} валидных файлов (исключены служебные)\")\n",
    "        return valid_files\n",
    "        \n",
    "    def find_all_parquet_files(self, folder_name):\n",
    "        \"\"\"Рекурсивный поиск всех parquet файлов в папке\"\"\"\n",
    "        folder_path = self.data_path / folder_name\n",
    "        if not folder_path.exists():\n",
    "            print(f\"✗ Папка {folder_name} не существует\")\n",
    "            return []\n",
    "            \n",
    "        # Рекурсивно ищем все .parquet файлов\n",
    "        parquet_files = list(folder_path.rglob(\"*.parquet\"))\n",
    "        print(f\"{folder_name}: найдено {len(parquet_files)} .parquet файлов\")\n",
    "        \n",
    "        # Фильтруем только валидные файлы\n",
    "        valid_files = self.filter_valid_parquet_files(parquet_files)\n",
    "        \n",
    "        # Показываем структуру\n",
    "        for i, file_path in enumerate(valid_files[:3]):\n",
    "            size_mb = file_path.stat().st_size / 1024 / 1024\n",
    "            print(f\"   {i+1}. {file_path.name} ({size_mb:.1f} MB)\")\n",
    "        if len(valid_files) > 3:\n",
    "            print(f\"   ... и еще {len(valid_files) - 3} файлов\")\n",
    "            \n",
    "        return valid_files\n",
    "        \n",
    "    def safe_read_parquet(self, file_path, n_rows=None, columns=None):\n",
    "        \"\"\"Безопасное чтение parquet файла\"\"\"\n",
    "        try:\n",
    "            size_mb = file_path.stat().st_size / 1024 / 1024\n",
    "            print(f\"   Чтение: {file_path.name} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            # Читаем сначала схему для отладки\n",
    "            schema = pl.read_parquet(file_path, n_rows=0).schema\n",
    "            print(f\"   Схема: {list(schema.keys())}\")\n",
    "            \n",
    "            # Используем use_pyarrow=False для совместимости с n_rows\n",
    "            df = pl.read_parquet(\n",
    "                file_path,\n",
    "                n_rows=n_rows,\n",
    "                columns=columns,\n",
    "                use_pyarrow=False,\n",
    "                low_memory=True\n",
    "            )\n",
    "            print(f\"  Успешно: {len(df)} строк, {len(df.columns)} столбцов\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка чтения {file_path.name}: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def read_multiple_parquet_files(self, file_paths, n_rows=None, columns=None, max_files=3):\n",
    "        \"\"\"Чтение нескольких parquet файлов с проверкой совместимости схем\"\"\"\n",
    "        if not file_paths:\n",
    "            return None\n",
    "            \n",
    "        print(f\"Чтение первых {min(len(file_paths), max_files)} файлов из {len(file_paths)}...\")\n",
    "        \n",
    "        all_dfs = []\n",
    "        first_schema = None\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths[:max_files]):\n",
    "            df = self.safe_read_parquet(file_path, n_rows, columns)\n",
    "            if df is not None:\n",
    "                # Проверяем совместимость схем\n",
    "                if first_schema is None:\n",
    "                    first_schema = df.schema\n",
    "                    all_dfs.append(df)\n",
    "                    print(f\"  Используем схему: {list(first_schema.keys())}\")\n",
    "                else:\n",
    "                    # Проверяем, совпадают ли схемы\n",
    "                    current_schema = df.schema\n",
    "                    if first_schema == current_schema:\n",
    "                        all_dfs.append(df)\n",
    "                    else:\n",
    "                        print(f\"  Пропускаем файл {file_path.name} - несовместимая схема\")\n",
    "                        print(f\"  Ожидалось: {list(first_schema.keys())}\")\n",
    "                        print(f\"  Получено: {list(current_schema.keys())}\")\n",
    "                        \n",
    "        if all_dfs:\n",
    "            try:\n",
    "                return pl.concat(all_dfs)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при объединении DataFrame: {e}\")\n",
    "                # Возвращаем первый DataFrame если объединение не удалось\n",
    "                return all_dfs[0] if all_dfs else None\n",
    "        return None\n",
    "        \n",
    "    def load_sample_data(self):\n",
    "        \"\"\"Загрузка выборки данных с рекурсивным поиском\"\"\"\n",
    "        print(\"Загрузка данных с рекурсивным поиском...\")\n",
    "        \n",
    "        datasets = [\n",
    "            ('test', \"ml_ozon_recsys_test_for_participants\", 5000, None, 2),\n",
    "            ('categories', \"ml_ozon_recsys_train_final_categories_tree\", None, None, 1),\n",
    "            ('items', \"ml_ozon_recsys_train_final_apparel_items_data\", 3000, \n",
    "             ['item_id', 'itemname', 'catalogid', 'variant_id', 'model_id'], 2),\n",
    "            ('orders', \"ml_ozon_recsys_train_final_apparel_orders_data\", 2000,\n",
    "             ['item_id', 'user_id', 'last_status', 'created_timestamp'], 2),\n",
    "            ('tracker', \"ml_ozon_recsys_train_final_apparel_tracker_data\", 1000,\n",
    "             ['item_id', 'user_id', 'timestamp', 'action_type'], 2)\n",
    "        ]\n",
    "        \n",
    "        for df_name, folder, n_rows, columns, max_files in datasets:\n",
    "            print(f\"\\nПоиск файлов в {folder}...\")\n",
    "            \n",
    "            # Находим все parquet файлы рекурсивно\n",
    "            parquet_files = self.find_all_parquet_files(folder)\n",
    "            \n",
    "            if parquet_files:\n",
    "                # Читаем несколько файлов\n",
    "                df = self.read_multiple_parquet_files(parquet_files, n_rows, columns, max_files)\n",
    "                if df is not None:\n",
    "                    self.dfs[df_name] = df\n",
    "                    print(f\"✓ {folder}: загружено {len(df)} строк из {max_files} файлов\")\n",
    "                else:\n",
    "                    print(f\"✗ {folder}: не удалось загрузить данные\")\n",
    "            else:\n",
    "                print(f\"✗ {folder}: parquet файлы не найдены\")\n",
    "            \n",
    "            gc.collect()\n",
    "    \n",
    "    def analyze_test_data(self):\n",
    "        \"\"\"Анализ тестовых данных\"\"\"\n",
    "        if 'test' not in self.dfs or self.dfs['test'] is None:\n",
    "            print(\"✗ Тестовые данные не загружены\")\n",
    "            return {}\n",
    "            \n",
    "        df = self.dfs['test']\n",
    "        print(\"Анализ тестовых данных.\")\n",
    "        \n",
    "        result = {\n",
    "            'rows': len(df),\n",
    "            'columns': df.columns,\n",
    "            'available_columns': list(df.columns)\n",
    "        }\n",
    "        \n",
    "        # Проверяем наличие столбцов перед анализом\n",
    "        if 'user_id' in df.columns:\n",
    "            result['unique_users'] = df['user_id'].n_unique()\n",
    "        \n",
    "        if 'item_id' in df.columns:\n",
    "            result['unique_items'] = df['item_id'].n_unique()\n",
    "        else:\n",
    "            print(\"  Столбец 'item_id' не найден в тестовых данных\")\n",
    "            result['unique_items'] = 'not_available'\n",
    "        \n",
    "        if 'created_timestamp' in df.columns:\n",
    "            result['time_range'] = {\n",
    "                'min': str(df['created_timestamp'].min()),\n",
    "                'max': str(df['created_timestamp'].max())\n",
    "            }\n",
    "        else:\n",
    "            print(\"  Столбец 'created_timestamp' не найден в тестовых данных\")\n",
    "            result['time_range'] = 'not_available'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def show_test_structure(self):\n",
    "        \"\"\"Показать структуру тестовых данных\"\"\"\n",
    "        if 'test' in self.dfs and self.dfs['test'] is not None:\n",
    "            df = self.dfs['test']\n",
    "            print(\"Структура тестовых данных:\")\n",
    "            print(f\"Колонки: {df.columns}\")\n",
    "            print(f\"Первые 5 строк:\")\n",
    "            print(df.head())\n",
    "            print(f\"Схема: {df.schema}\")\n",
    "        else:\n",
    "            print(\"Тестовые данные не загружены\")\n",
    "    \n",
    "    def analyze_items_data(self):\n",
    "        \"\"\"Анализ данных товаров\"\"\"\n",
    "        if 'items' not in self.dfs or self.dfs['items'] is None:\n",
    "            print(\"✗ Данные товаров не загружены\")\n",
    "            return {}\n",
    "            \n",
    "        df = self.dfs['items']\n",
    "        print(\"Анализ данных товаров.\")\n",
    "        return {\n",
    "            'rows': len(df),\n",
    "            'unique_categories': df['catalogid'].n_unique(),\n",
    "            'null_names': df['itemname'].null_count(),\n",
    "            'unique_names': df['itemname'].n_unique(),\n",
    "            'columns': df.columns\n",
    "        }\n",
    "    \n",
    "    def analyze_orders_data(self):\n",
    "        \"\"\"Анализ данных заказов\"\"\"\n",
    "        if 'orders' not in self.dfs or self.dfs['orders'] is None:\n",
    "            print(\"✗ Данные заказов не загружены\")\n",
    "            return {}\n",
    "            \n",
    "        df = self.dfs['orders']\n",
    "        print(\"Анализ данных заказов.\")\n",
    "        \n",
    "        result = {\n",
    "            'rows': len(df),\n",
    "            'columns': df.columns\n",
    "        }\n",
    "        \n",
    "        # Проверяем наличие обязательных столбцов\n",
    "        if 'last_status' in df.columns:\n",
    "            status_counts = {}\n",
    "            for status in df['last_status'].unique():\n",
    "                count = df.filter(pl.col('last_status') == status).height\n",
    "                status_counts[status] = count\n",
    "            \n",
    "            result['status_distribution'] = status_counts\n",
    "            result['delivered_orders'] = status_counts.get('delivered_orders', 0)\n",
    "            result['canceled_orders'] = status_counts.get('canceled_orders', 0)\n",
    "        else:\n",
    "            print(\"  Столбец 'last_status' не найден\")\n",
    "            result['status_distribution'] = 'not_available'\n",
    "        \n",
    "        if 'user_id' in df.columns:\n",
    "            result['unique_users'] = df['user_id'].n_unique()\n",
    "        else:\n",
    "            result['unique_users'] = 'not_available'\n",
    "        \n",
    "        if 'item_id' in df.columns:\n",
    "            result['unique_items'] = df['item_id'].n_unique()\n",
    "        else:\n",
    "            result['unique_items'] = 'not_available'\n",
    "        \n",
    "        if 'created_timestamp' in df.columns:\n",
    "            result['time_range'] = {\n",
    "                'min': str(df['created_timestamp'].min()),\n",
    "                'max': str(df['created_timestamp'].max())\n",
    "            }\n",
    "        else:\n",
    "            result['time_range'] = 'not_available'\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def analyze_tracker_data(self):\n",
    "        \"\"\"Анализ данных трекера\"\"\"\n",
    "        if 'tracker' not in self.dfs or self.dfs['tracker'] is None:\n",
    "            print(\"✗ Данные трекера не загружены\")\n",
    "            return {}\n",
    "            \n",
    "        df = self.dfs['tracker']\n",
    "        print(\"Анализ данных трекера.\")\n",
    "        \n",
    "        result = {\n",
    "            'rows': len(df),\n",
    "            'columns': df.columns\n",
    "        }\n",
    "        \n",
    "        if 'action_type' in df.columns:\n",
    "            action_counts = {}\n",
    "            for action in df['action_type'].unique():\n",
    "                count = df.filter(pl.col('action_type') == action).height\n",
    "                action_counts[action] = count\n",
    "            \n",
    "            result['action_distribution'] = action_counts\n",
    "        else:\n",
    "            print(\"  Столбец 'action_type' не найден\")\n",
    "            result['action_distribution'] = 'not_available'\n",
    "        \n",
    "        if 'user_id' in df.columns:\n",
    "            result['unique_users'] = df['user_id'].n_unique()\n",
    "        else:\n",
    "            result['unique_users'] = 'not_available'\n",
    "        \n",
    "        if 'item_id' in df.columns:\n",
    "            result['unique_items'] = df['item_id'].n_unique()\n",
    "        else:\n",
    "            result['unique_items'] = 'not_available'\n",
    "        \n",
    "        if 'timestamp' in df.columns:\n",
    "            result['time_range'] = {\n",
    "                'min': str(df['timestamp'].min()),\n",
    "                'max': str(df['timestamp'].max())\n",
    "            }\n",
    "        else:\n",
    "            result['time_range'] = 'not_available'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_categories_data(self):\n",
    "        \"\"\"Анализ дерева категорий\"\"\"\n",
    "        if 'categories' not in self.dfs or self.dfs['categories'] is None:\n",
    "            print(\"✗ Данные категорий не загружены\")\n",
    "            return {}\n",
    "            \n",
    "        df = self.dfs['categories']\n",
    "        print(\"Анализ дерева категорий.\")\n",
    "        \n",
    "        # Безопасное вычисление максимальной глубины\n",
    "        max_depth = 0\n",
    "        for ids in df['ids']:\n",
    "            if ids is not None:\n",
    "                max_depth = max(max_depth, len(ids))\n",
    "        \n",
    "        return {\n",
    "            'rows': len(df),\n",
    "            'total_categories': df['catalogid'].n_unique(),\n",
    "            'max_depth': max_depth,\n",
    "            'columns': df.columns\n",
    "        }\n",
    "    \n",
    "    def create_basic_visualizations(self):\n",
    "        \"\"\"Создание отдельных визуализаций для каждого доступного типа данных\"\"\"\n",
    "        try:\n",
    "            created_plots = 0\n",
    "            \n",
    "            # 1. Распределение статусов заказов\n",
    "            if ('orders' in self.dfs and self.dfs['orders'] is not None and \n",
    "                'last_status' in self.dfs['orders'].columns):\n",
    "                \n",
    "                df = self.dfs['orders']\n",
    "                status_counts = {}\n",
    "                for status in df['last_status'].unique():\n",
    "                    count = df.filter(pl.col('last_status') == status).height\n",
    "                    status_counts[status] = count\n",
    "                \n",
    "                if status_counts:\n",
    "                    plt.figure(figsize=(10, 7))\n",
    "                    labels = list(status_counts.keys())\n",
    "                    sizes = list(status_counts.values())\n",
    "                    plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "                    plt.title('Распределение статусов заказов')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('orders_status_distribution.png', dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    created_plots += 1\n",
    "                    print(\"Создана визуализация статусов заказов\")\n",
    "            \n",
    "            # 2. Распределение действий в трекере\n",
    "            if ('tracker' in self.dfs and self.dfs['tracker'] is not None and \n",
    "                'action_type' in self.dfs['tracker'].columns):\n",
    "                \n",
    "                df = self.dfs['tracker']\n",
    "                action_counts = {}\n",
    "                for action in df['action_type'].unique():\n",
    "                    count = df.filter(pl.col('action_type') == action).height\n",
    "                    action_counts[action] = count\n",
    "                \n",
    "                if action_counts:\n",
    "                    plt.figure(figsize=(10, 7))\n",
    "                    # Берем топ-5 действий\n",
    "                    sorted_actions = sorted(action_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                    actions, counts = zip(*sorted_actions)\n",
    "                    \n",
    "                    plt.bar(range(len(actions)), counts)\n",
    "                    plt.xticks(range(len(actions)), actions, rotation=45)\n",
    "                    plt.title('Топ-5 типов действий в трекере')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('tracker_actions_distribution.png', dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    created_plots += 1\n",
    "                    print(\"Создана визуализация действий трекера\")\n",
    "            \n",
    "            if created_plots == 0:\n",
    "                print(\"Недостаточно данных для визуализаций\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"Создано {created_plots} визуализаций\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка создания визуализаций: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def design_features(self):\n",
    "        \"\"\"Проектирование фич для рекомендательной системы\"\"\"\n",
    "        feature_plan = {\n",
    "            'user_features': [\n",
    "                'user_total_events', 'user_unique_items', 'user_activity_recency',\n",
    "                'user_buying_frequency', 'user_cart_add_rate', 'user_purchase_rate'\n",
    "            ],\n",
    "            'item_features': [\n",
    "                'item_popularity', 'item_recent_popularity', 'item_conversion_rate',\n",
    "                'item_category', 'item_price_tier', 'item_age'\n",
    "            ],\n",
    "            'user_item_features': [\n",
    "                'user_item_view_count', 'user_item_time_since_last_view',\n",
    "                'user_item_in_cart', 'user_item_in_wishlist'\n",
    "            ],\n",
    "            'context_features': [\n",
    "                'time_of_day', 'day_of_week', 'season', 'is_weekend'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return feature_plan\n",
    "    \n",
    "    def generate_feature_engineering_plan(self):\n",
    "        \"\"\"Генерация плана feature engineering\"\"\"\n",
    "        return {\n",
    "            'steps': [\n",
    "                '1. Подготовка данных и очистка',\n",
    "                '2. Генерация user features из tracker данных',\n",
    "                '3. Генерация item features из items и tracker данных', \n",
    "                '4. Создание user-item interaction features',\n",
    "                '5. Добавление временных и контекстных features',\n",
    "                '6. Negative sampling и создание целевой переменной'\n",
    "            ],\n",
    "            'recommendations': [\n",
    "                'Использовать Polars для эффективной обработки',\n",
    "                'Применить временную валидацию',\n",
    "                'Использовать инкрементальную обработку для больших данных'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def get_summary_statistics(self):\n",
    "        \"\"\"Получение сводной статистики\"\"\"\n",
    "        # Фильтруем только не-None датасеты\n",
    "        valid_datasets = {name: df for name, df in self.dfs.items() if df is not None}\n",
    "        \n",
    "        return {\n",
    "            'datasets': {name: len(df) for name, df in valid_datasets.items()},\n",
    "            'memory_usage_mb': psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024,\n",
    "            'loaded_datasets_count': len(valid_datasets),\n",
    "            'total_rows': sum(len(df) for df in valid_datasets.values())\n",
    "        }\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Полный анализ данных\"\"\"\n",
    "        print(\"Запуск полного анализа данных с рекурсивным поиском\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. Загрузка данных\n",
    "            self.load_sample_data()\n",
    "            \n",
    "            # 2. Анализ каждого датасета\n",
    "            self.results['test_analysis'] = self.analyze_test_data()\n",
    "            self.results['items_analysis'] = self.analyze_items_data()\n",
    "            self.results['orders_analysis'] = self.analyze_orders_data()\n",
    "            self.results['tracker_analysis'] = self.analyze_tracker_data()\n",
    "            self.results['categories_analysis'] = self.analyze_categories_data()\n",
    "            self.results['test_analysis'] = self.analyze_test_data()\n",
    "            \n",
    "            # 3. Визуализации\n",
    "            self.results['visualizations_created'] = self.create_basic_visualizations()\n",
    "            \n",
    "            # 4. Проектирование фич\n",
    "            self.results['feature_plan'] = self.design_features()\n",
    "            self.results['engineering_plan'] = self.generate_feature_engineering_plan()\n",
    "            \n",
    "            # 5. Сводная статистика\n",
    "            summary = self.get_summary_statistics()\n",
    "            self.results['summary'] = {\n",
    "                'total_datasets_loaded': summary['loaded_datasets_count'],\n",
    "                'total_rows_analyzed': summary['total_rows'],\n",
    "                'analysis_timestamp': datetime.now().isoformat(),\n",
    "                'memory_usage_mb': summary['memory_usage_mb']\n",
    "            }\n",
    "            \n",
    "            print(\"Анализ успешно завершен!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка во время анализа: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Добавляем фикс для AVX проблемы\n",
    "    import os\n",
    "    os.environ['POLARS_SKIP_CPU_CHECK'] = '1'\n",
    "    \n",
    "    analyzer = OzonDataAnalyzer(\"/home/dima/ozon/data\")\n",
    "    results = analyzer.run_complete_analysis()\n",
    "    \n",
    "    # Сохранение результатов\n",
    "    with open('eda_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Результаты сохранены в eda_results.json\")\n",
    "    \n",
    "    # Вывод сводной информации\n",
    "    if 'summary' in results:\n",
    "        summary = results['summary']\n",
    "        print(f\"\\nСводная статистика:\")\n",
    "        print(f\"Проанализировано датасетов: {summary.get('total_datasets_loaded', 0)}\") # type: ignore\n",
    "        print(f\"Общее количество строк: {summary.get('total_rows_analyzed', 0)}\") # type: ignore\n",
    "        print(f\"Пиковое использование памяти: {summary.get('memory_usage_mb', 0):.1f} MB\") # type: ignore\n",
    "    else:\n",
    "        print(\"Не удалось получить сводную статистику\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9e355",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
