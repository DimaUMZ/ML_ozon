#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–∏—á–µ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã OZON
"""

import pandas as pd
import numpy as np
import json
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(root_dir)

class DataLoader:
    def __init__(self, config_path='config/feature_config.json'):
        # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É
        config_full_path = os.path.join(root_dir, config_path)
        self.config = self._load_config(config_full_path)
        self.data_paths = self.config.get('data_paths', {})
        
        print(f"üìÅ Config loaded from: {config_full_path}")
    
    def _load_config(self, config_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ùå Config file {config_path} not found. Using default paths.")
            return {
                'data_paths': {
                    'raw_interactions': 'data/ml_ozon_recsys_test_for_participants/ml_ozon_recsys_train_final_apparel_tracker_data/',
                    'raw_items': 'data/ml_ozon_recsys_test_for_participants/ml_ozon_recsys_train_final_apparel_items_data/',
                    'raw_orders': 'data/ml_ozon_recsys_test_for_participants/ml_ozon_recsys_train_final_apparel_orders_data/',
                    'processed_features': 'data/processed/features.parquet',
                    'processed_targets': 'data/processed/targets.parquet'
                }
            }
    
    def load_interactions(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–µ—Ä–∞ —Å fallback –Ω–∞ orders"""
        tracker_path = self.data_paths.get('raw_interactions', '')
        full_path = os.path.join(root_dir, tracker_path)
        
        result = self._load_parquet_data(full_path, "tracker interactions")
        
        # –ï—Å–ª–∏ tracker –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º orders –∫–∞–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        if result.empty:
            print("‚ö†Ô∏è Tracker data not found, trying to use orders as interactions...")
            orders_path = self.data_paths.get('raw_orders', '')
            orders_full_path = os.path.join(root_dir, orders_path)
            orders_data = self._load_parquet_data(orders_full_path, "orders")
            
            if not orders_data.empty:
                print("‚úÖ Using orders data as interactions")
                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                if 'user_id' in orders_data.columns and 'item_id' in orders_data.columns:
                    result = orders_data.copy()
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
                    result['action'] = 'purchase'
                    result['timestamp'] = pd.to_datetime('2025-01-01')  # –§–∏–∫—Ç–∏–≤–Ω–∞—è –¥–∞—Ç–∞
                else:
                    print("‚ùå Orders data doesn't have required columns")
        
        return result
    
    def load_items(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö"""
        items_path = self.data_paths.get('raw_items', '')
        full_path = os.path.join(root_dir, items_path)
        return self._load_parquet_data(full_path, "items")
    
    def load_orders(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–∫–∞–∑–∞—Ö"""
        orders_path = self.data_paths.get('raw_orders', '')
        full_path = os.path.join(root_dir, orders_path)
        return self._load_parquet_data(full_path, "orders")
    
    def load_users(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö"""
        print("‚ÑπÔ∏è User data not specified, returning empty DataFrame")
        return pd.DataFrame()
    
    def _load_parquet_data(self, data_path, data_type):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ parquet –¥–∞–Ω–Ω—ã—Ö, –∏–≥–Ω–æ—Ä–∏—Ä—É—è macOS —Ñ–∞–π–ª—ã"""
        print(f"üîç Looking for {data_type} in: {data_path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—É—Ç–∏
        if not os.path.exists(data_path):
            print(f"‚ùå Path does not exist: {data_path}")
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–∏—Ö parquet —Ñ–∞–π–ª–æ–≤
            print("üîç Starting recursive search for parquet files...")
            parquet_files = self._find_parquet_files_recursive(root_dir)
            
            if parquet_files:
                print(f"‚úÖ Found {len(parquet_files)} valid parquet files")
                
                # –ò—â–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö
                matching_files = []
                for file in parquet_files:
                    file_lower = file.lower()
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã macOS –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    if ('__macosx' in file_lower or 
                        '/.' in file or 
                        file.startswith('._') or 
                        'cache' in file_lower):
                        continue
                    
                    # –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
                    if data_type.lower() == "tracker interactions":
                        if any(keyword in file_lower for keyword in ['tracker', 'interaction', 'click', 'view', 'session']):
                            matching_files.append(file)
                    elif data_type.lower() == "items":
                        if any(keyword in file_lower for keyword in ['item', 'product', 'catalog', 'goods']):
                            matching_files.append(file)
                    elif data_type.lower() == "orders":
                        if any(keyword in file_lower for keyword in ['order', 'purchase', 'buy', 'transaction']):
                            matching_files.append(file)
                
                if matching_files:
                    print(f"üìÅ Found {len(matching_files)} matching files for {data_type}")
                    for i, file in enumerate(matching_files[:5]):  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 5
                        print(f"   {i+1}. {file}")
                    
                    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã –ø–æ –ø–æ—Ä—è–¥–∫—É
                    for file in matching_files:
                        print(f"üìÅ Trying to load: {file}")
                        try:
                            df = pd.read_parquet(file)
                            print(f"‚úÖ Successfully loaded {len(df)} rows from {file}")
                            return df
                        except Exception as e:
                            print(f"‚ùå Failed to load {file}: {e}")
                            continue
                
                print(f"‚ö†Ô∏è No matching files found for {data_type}")
            else:
                print("‚ùå No parquet files found anywhere in the project")
            
            return pd.DataFrame()
        
        # –ï—Å–ª–∏ –ø—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.isdir(data_path):
            # –ò—â–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–∏–µ parquet —Ñ–∞–π–ª—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º macOS)
            files = [f for f in os.listdir(data_path) 
                    if f.endswith('.parquet') and not f.startswith('._')]
            
            if files:
                print(f"‚úÖ Found {len(files)} parquet files in directory")
                for file in files:
                    print(f"   - {file}")
                
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É
                for file in files:
                    file_path = os.path.join(data_path, file)
                    print(f"üìÅ Trying to load: {file_path}")
                    try:
                        df = pd.read_parquet(file_path)
                        print(f"‚úÖ Successfully loaded {len(df)} rows from {file}")
                        return df
                    except Exception as e:
                        print(f"‚ùå Failed to load {file_path}: {e}")
                        continue
                
                print(f"‚ùå All files in {data_path} failed to load")
            else:
                print(f"‚ùå No valid parquet files found in: {data_path}")
        
        elif os.path.isfile(data_path) and data_path.endswith('.parquet'):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ macOS —Ñ–∞–π–ª
            if os.path.basename(data_path).startswith('._'):
                print(f"‚ùå Skipping macOS metadata file: {data_path}")
                return pd.DataFrame()
            
            try:
                df = pd.read_parquet(data_path)
                print(f"‚úÖ Loaded {len(df)} {data_type} from file")
                return df
            except Exception as e:
                print(f"‚ùå Error reading {data_path}: {e}")
        
        return pd.DataFrame()
    
    def _find_parquet_files_recursive(self, search_path):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ parquet —Ñ–∞–π–ª–æ–≤, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã macOS"""
        parquet_files = []
        
        if not os.path.exists(search_path):
            return parquet_files
        
        try:
            for root, dirs, files in os.walk(search_path):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ macOS —Å–ª—É–∂–µ–±–Ω—ã–µ
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__MACOSX']
                
                for file in files:
                    if (file.endswith('.parquet') and 
                        not file.startswith('._') and  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º macOS –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        not root.endswith('/__MACOSX')):  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º macOS –ø–∞–ø–∫–∏
                        
                        full_path = os.path.join(root, file)
                        parquet_files.append(full_path)
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Error during search: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—É—Ç–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        parquet_files.sort()
        return parquet_files
    
    def merge_datasets(self, interactions, items, orders, users):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ datasets"""
        if interactions.empty:
            print("‚ùå No interactions data to merge")
            return pd.DataFrame()
        
        merged = interactions.copy()
        print(f"üìä Initial interactions: {merged.shape}")
        print(f"üìã Interactions columns: {list(merged.columns)}")
        
        # –ú–µ—Ä–¥–∂ —Å —Ç–æ–≤–∞—Ä–∞–º–∏
        if not items.empty and 'item_id' in interactions.columns and 'item_id' in items.columns:
            merged = merged.merge(
                items, 
                on='item_id', 
                how='left',
                suffixes=('', '_item')
            )
            print(f"üìä Merged with items: {merged.shape}")
        else:
            print("‚ö†Ô∏è Items merge skipped")
        
        # –ú–µ—Ä–¥–∂ —Å –∑–∞–∫–∞–∑–∞–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –æ–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏)
        if not orders.empty:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –º–µ—Ä–¥–∂–∞
            common_columns = set(merged.columns) & set(orders.columns)
            print(f"üîç Common columns with orders: {common_columns}")
            
            if 'order_id' in common_columns:
                merged = merged.merge(
                    orders,
                    on='order_id',
                    how='left',
                    suffixes=('', '_order')
                )
                print(f"üìä Merged with orders: {merged.shape}")
            elif 'user_id' in common_columns and 'item_id' in common_columns:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ä–¥–∂ –ø–æ user_id + item_id
                merged = merged.merge(
                    orders,
                    on=['user_id', 'item_id'],
                    how='left',
                    suffixes=('', '_order')
                )
                print(f"üìä Merged with orders by user+item: {merged.shape}")
            else:
                print("‚ö†Ô∏è Orders merge skipped - no common columns")
        
        # –ú–µ—Ä–¥–∂ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏
        if not users.empty and 'user_id' in merged.columns and 'user_id' in users.columns:
            merged = merged.merge(
                users,
                on='user_id',
                how='left',
                suffixes=('', '_user')
            )
            print(f"üìä Merged with users: {merged.shape}")
        
        print(f"üìã Final columns: {list(merged.columns)}")
        return merged
    
    def save_processed_data(self, features, targets=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            processed_features_path = os.path.join(root_dir, self.data_paths['processed_features'])
            os.makedirs(os.path.dirname(processed_features_path), exist_ok=True)
            
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            columns_to_drop = ['fclip_embed', 'attributes']  # –ö–æ–ª–æ–Ω–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–∑—ã–≤–∞—é—Ç –æ—à–∏–±–∫–∏
            features_to_save = features.drop(columns=[col for col in columns_to_drop 
                                                    if col in features.columns])
            
            print(f"üìä Saving {features_to_save.shape[1]} columns (removed problematic columns)")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏—á–∏
            features_to_save.to_parquet(processed_features_path)
            print(f"üíæ Features saved to: {processed_features_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if targets is not None:
                processed_targets_path = os.path.join(root_dir, self.data_paths['processed_targets'])
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Series –≤ DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                targets_df = pd.DataFrame({'target': targets})
                targets_df.to_parquet(processed_targets_path)
                print(f"üíæ Targets saved to: {processed_targets_path}")
                print(f"   Targets shape: {targets_df.shape}")
                
        except Exception as e:
            print(f"‚ùå Error saving data: {e}")
            import traceback
            traceback.print_exc()

class FeatureEngineer:
    """–ü—Ä–æ—Å—Ç–æ–π –∏–Ω–∂–µ–Ω–µ—Ä —Ñ–∏—á–µ–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    @staticmethod
    def create_basic_features(df):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á–µ–π"""
        if df.empty:
            return df
        
        df = df.copy()
        print("üõ†Ô∏è Creating basic features...")
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        if 'timestamp' in df.columns:
            try:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df['hour'] = df['timestamp'].dt.hour
                df['day_of_week'] = df['timestamp'].dt.dayofweek
                df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
                print("‚úÖ Temporal features created")
            except Exception as e:
                print(f"‚ùå Error creating temporal features: {e}")
        
        # User features
        if 'user_id' in df.columns:
            user_counts = df['user_id'].value_counts()
            df['user_activity_count'] = df['user_id'].map(user_counts)
            print("‚úÖ User features created")
        
        # Item features
        if 'item_id' in df.columns:
            item_counts = df['item_id'].value_counts()
            df['item_popularity'] = df['item_id'].map(item_counts)
            print("‚úÖ Item features created")
        
        print(f"üõ†Ô∏è Final features shape: {df.shape}")
        return df
    
    @staticmethod
    def create_targets(df, action_column='action_type', positive_actions=['buy', 'purchase', 'cart', 'order']):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        if df.empty or action_column not in df.columns:
            print(f"‚ö†Ô∏è Cannot create targets - no '{action_column}' column")
            print(f"   Available columns: {list(df.columns)}")
            return None
        
        # –ü–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        unique_actions = df[action_column].astype(str).unique()
        print(f"üîç Unique actions in data: {unique_actions}")
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        actions = df[action_column].astype(str).str.lower()
        targets = actions.isin([a.lower() for a in positive_actions]).astype(int)
        
        positive_count = targets.sum()
        total_count = len(targets)
        
        print(f"üéØ Targets created from '{action_column}': {positive_count}/{total_count} positive samples ({positive_count/total_count:.1%})")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö samples, –ø–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        if positive_count == 0:
            print("‚ö†Ô∏è No positive samples found. Trying alternative actions...")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ –Ω–µ-view –¥–µ–π—Å—Ç–≤–∏—è
            non_view_actions = [action for action in unique_actions 
                            if 'view' not in str(action).lower() and 'click' not in str(action).lower()]
            if non_view_actions:
                print(f"üîç Trying alternative actions: {non_view_actions[:3]}")
                targets = actions.isin([str(a).lower() for a in non_view_actions]).astype(int)
                positive_count = targets.sum()
                print(f"üéØ Alternative targets: {positive_count}/{total_count} positive samples ({positive_count/total_count:.1%})")
        
        return targets

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–∏—á–µ–π"""
    print("=" * 50)
    print("üöÄ STARTING OZON FEATURE ENGINEERING PIPELINE")
    print("=" * 50)
    
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        data_loader = DataLoader()
        feature_engineer = FeatureEngineer()
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n1. üì• LOADING DATA")
        print("-" * 30)
        
        interactions = data_loader.load_interactions()
        items = data_loader.load_items()
        orders = data_loader.load_orders()
        users = data_loader.load_users()
        print(f"üîç Actions in interactions: {interactions['action_type'].unique()}")
        print(f"üìä Action counts:")
        print(interactions['action_type'].value_counts())
        
        print(f"   üìä Interactions: {interactions.shape}")
        print(f"   üì¶ Items: {items.shape}")
        print(f"   üõí Orders: {orders.shape}")
        print(f"   üë• Users: {users.shape}")
        
        # 2. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        print("\n2. üîÑ MERGING DATASETS")
        print("-" * 30)
        
        merged_data = data_loader.merge_datasets(interactions, items, orders, users)
        
        if merged_data.empty:
            print("‚ùå No data to process. Exiting.")
            return None
        
        print(f"   üìä Merged data: {merged_data.shape}")
        print(f"   üìã Columns: {list(merged_data.columns)}")
        
        # 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏—á–µ–π
        print("\n3. üõ†Ô∏è FEATURE ENGINEERING")
        print("-" * 30)
        
        features = feature_engineer.create_basic_features(merged_data)
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        print("\n4. üéØ CREATING TARGETS")
        print("-" * 30)
        
        targets = feature_engineer.create_targets(features)
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n5. üíæ SAVING RESULTS")
        print("-" * 30)
        
        data_loader.save_processed_data(features, targets)
        
        print("\n" + "=" * 50)
        print("‚úÖ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        
        return features, targets
        
    except Exception as e:
        print(f"\n‚ùå ERROR IN PIPELINE: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()