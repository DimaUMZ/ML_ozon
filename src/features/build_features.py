#!/usr/bin/env python3
"""
Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ñ‡ĞµĞ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ OZON
"""

import pandas as pd
import numpy as np
import json
import os
import sys
from pathlib import Path

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ñ€Ğ½ĞµĞ²ÑƒÑ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² Ğ¿ÑƒÑ‚ÑŒ
current_dir = Path(__file__).parent
root_dir = current_dir.parent.parent
sys.path.append(str(root_dir))

# Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ñ‡ĞµÑ€ĞµĞ· src, Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
import importlib.util

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸
def import_module_from_path(module_name, file_path):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec) # type: ignore
    spec.loader.exec_module(module) # type: ignore
    return module

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°ÑˆĞ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸
ozon_loader_path = root_dir / 'src' / 'features' / 'ozon_data_loader.py'
feature_engineer_path = root_dir / 'src' / 'features' / 'enhanced_feature_engineer.py'

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
print(f"ğŸ” Checking loader path: {ozon_loader_path}")
print(f"ğŸ” Checking engineer path: {feature_engineer_path}")

if ozon_loader_path.exists() and feature_engineer_path.exists():
    OzonDataLoader = import_module_from_path('ozon_data_loader', ozon_loader_path).OzonDataLoader
    EnhancedFeatureEngineer = import_module_from_path('enhanced_feature_engineer', feature_engineer_path).EnhancedFeatureEngineer
    print("âœ… Modules imported successfully")
else:
    print("âŒ Module files not found, creating simple implementation...")
    
    # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞµÑĞ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹
    class OzonDataLoader:
        def __init__(self, config_path='config/feature_config.json'):
            self.root_dir = Path(__file__).parent.parent.parent
            print(f"ğŸ“ Project root: {self.root_dir}")
        
        def explore_data_structure(self):
            """Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸"""
            print("ğŸ” Exploring data directory structure...")
            data_dir = self.root_dir / 'data'
            if data_dir.exists():
                print("âœ… Data directory exists")
                for item in data_dir.rglob("*.parquet"):
                    if not item.name.startswith('._'):
                        print(f"   ğŸ“„ {item.relative_to(self.root_dir)}")
            else:
                print("âŒ Data directory does not exist")
        
        def load_interactions(self):
            """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ"""
            print("ğŸ“ Creating sample interactions data...")
            return pd.DataFrame({
                'user_id': [1, 1, 2, 2, 3, 3, 4, 4, 5, 5],
                'item_id': [101, 102, 101, 103, 102, 104, 103, 105, 104, 106],
                'timestamp': pd.date_range('2025-01-01', periods=10, freq='H'),
                'action_type': ['view', 'purchase', 'view', 'view', 'purchase', 'view', 'cart', 'view', 'purchase', 'view']
            })
    
    class EnhancedFeatureEngineer:
        def __init__(self, config=None):
            self.config = config or {}
            self.time_windows = self.config.get('time_windows', [1, 3, 7, 14, 30])
        
        def create_enhanced_features(self, df):
            """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ñ‡ĞµĞ¹"""
            if df.empty:
                return df
            
            df = df.copy()
            print("ğŸ› ï¸ Creating enhanced features...")
            
            # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ñ‡Ğ¸
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df['hour'] = df['timestamp'].dt.hour
                df['day_of_week'] = df['timestamp'].dt.dayofweek
                df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
            
            # User features
            if 'user_id' in df.columns:
                user_stats = df.groupby('user_id').agg({
                    'timestamp': ['count', 'min', 'max']
                }).reset_index()
                user_stats.columns = ['user_id', 'user_total_events', 'first_activity', 'last_activity']
                df = df.merge(user_stats, on='user_id', how='left')
            
            # Item features
            if 'item_id' in df.columns:
                item_stats = df.groupby('item_id').agg({
                    'timestamp': 'count',
                    'user_id': 'nunique'
                }).reset_index()
                item_stats.columns = ['item_id', 'item_popularity', 'unique_users']
                df = df.merge(item_stats, on='item_id', how='left')
            
            return df
        
        def create_improved_targets(self, df):
            """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ¾Ğ²"""
            if 'action_type' not in df.columns:
                return None
            
            # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ¾Ğ²
            positive_actions = ['purchase', 'buy', 'cart', 'order']
            df['action_lower'] = df['action_type'].astype(str).str.lower()
            targets = df['action_lower'].isin(positive_actions).astype(int)
            
            print(f"ğŸ¯ Targets: {targets.sum()}/{len(targets)} positive ({targets.mean():.2%})")
            return targets

        def optimize_datasets_dtypes(self, df, verbose=True):
            """ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
            if df.empty:
                print("âŒ DataFrame is empty - skipping optimization")
                return df
        
            optimized_df = df.copy()
            total_memory_before = optimized_df.memory_usage(deep=True).sum()
            changes_made = 0
        
            print(f"ğŸ§® Initial memory usage: {total_memory_before / 1024 ** 2:.2f} MB")
            print(f"ğŸ“Š Initial shape: {optimized_df.shape}")
        
            for column in optimized_df.columns:
                current_dtype = optimized_df[column].dtype
                memory_before = optimized_df[column].memory_usage(deep=True)
            
                # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ datetime ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸
                if pd.api.types.is_datetime64_any_dtype(current_dtype):
                    if verbose:
                        print(f"   â° {column}: {current_dtype} (datetime - skipped)")
                    continue
            
                # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                if current_dtype == object and optimized_df[column].apply(
                    lambda x: isinstance(x, (list, dict, np.ndarray))).any():
                    if verbose:
                        print(f"   ğŸ”„ {column}: complex type (list/dict/array - skipped)")
                    continue
            
                new_dtype = current_dtype
             
                # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
                if pd.api.types.is_numeric_dtype(current_dtype):
                    col_min = optimized_df[column].min()
                    col_max = optimized_df[column].max()
                
                    if pd.api.types.is_integer_dtype(current_dtype):
                        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                            new_dtype = 'int8'
                        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                            new_dtype = 'int16'
                        elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                            new_dtype = 'int32'
            
                # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
                elif current_dtype == 'object':
                    unique_ratio = optimized_df[column].nunique() / len(optimized_df[column])
                    if unique_ratio < 0.5:  # ĞœĞµĞ½ÑŒÑˆĞµ 50% ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
                        new_dtype = 'category'
            
                # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ° ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
                if str(new_dtype) != str(current_dtype):
                    try:
                        original_values = optimized_df[column].copy()
                        optimized_df[column] = optimized_df[column].astype(new_dtype)
                    
                        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                        if pd.api.types.is_numeric_dtype(current_dtype):
                            if not np.array_equal(original_values, optimized_df[column], equal_nan=True):
                                optimized_df[column] = original_values
                                if verbose:
                                    print(f"   âš ï¸ {column}: data integrity check failed - reverted")
                                continue
                    
                        memory_after = optimized_df[column].memory_usage(deep=True)
                        memory_saved = memory_before - memory_after
                        changes_made += 1
                    
                        if verbose:
                            print(f"   âœ… {column}: {current_dtype} â†’ {new_dtype} "
                                f"(saved: {memory_saved / 1024:.1f} KB)")
                        
                    except (ValueError, TypeError) as e:
                        if verbose:
                            print(f"   âŒ {column}: conversion failed {current_dtype} â†’ {new_dtype} ({str(e)})")
        
            # Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
            total_memory_after = optimized_df.memory_usage(deep=True).sum()
            total_memory_saved = total_memory_before - total_memory_after
        
            print(f"ğŸ“ˆ Final memory usage: {total_memory_after / 1024 ** 2:.2f} MB")
            print(f"ğŸ’¾ Total memory saved: {total_memory_saved / 1024 ** 2:.2f} MB")
            print(f"ğŸ”§ Columns optimized: {changes_made}/{len(df.columns)}")
        
            return optimized_df

def explore_directory():
    """Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸"""
    print("ğŸ” Exploring directory structure...")
    
    root_dir = Path(__file__).parent.parent.parent
    data_dir = root_dir / 'data'
    print(f"ğŸ“ Root: {root_dir}")
    print(f"ğŸ“ Data: {data_dir}")
    
    if data_dir.exists():
        print("âœ… Data directory exists")
        parquet_files = list(data_dir.rglob("*.parquet"))
        for file in parquet_files[:10]:  # ĞŸĞ¾ĞºĞ°Ğ¶ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 10 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
            if not file.name.startswith('._'):
                print(f"   ğŸ“„ {file.relative_to(root_dir)}")
        if len(parquet_files) > 10:
            print(f"   ... and {len(parquet_files) - 10} more")
    else:
        print("âŒ Data directory does not exist")

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ñ‡ĞµĞ¹"""
    print("=" * 60)
    print("ğŸš€ STARTING OZON FEATURE ENGINEERING PIPELINE")
    print("=" * 60)
    
    try:
        # 0. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹
        print("\n0. ğŸ” DIRECTORY EXPLORATION")
        print("-" * 40)
        explore_directory()
        
        # 1. Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        print("\n1. ğŸ“¥ INITIALIZATION")
        print("-" * 40)
        
        data_loader = OzonDataLoader()
        
        # 2. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        print("\n2. ğŸ“¥ LOADING DATA")
        print("-" * 40)
        
        data_loader.explore_data_structure()
        interactions = data_loader.load_interactions()
        
        print(f"\nğŸ“Š Interactions shape: {interactions.shape}")
        print(f"ğŸ“‹ Interactions columns: {list(interactions.columns)}")
        
        if not interactions.empty and 'action_type' in interactions.columns:
            print(f"ğŸ” Action types: {interactions['action_type'].unique()}")
        
        # 3. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ°
        print("\n3. âš™ï¸ LOADING CONFIG")
        print("-" * 40)
        
        root_dir = Path(__file__).parent.parent.parent
        config_path = root_dir / 'config' / 'feature_config.json'
        
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
            print("âœ… Config loaded successfully")
        else:
            print("âš ï¸ Config file not found, using defaults")
            config = {
                'feature_params': {
                    'time_windows': [1, 3, 7, 14, 30],
                    'min_user_interactions': 3
                }
            }
        
        # 4. Feature Engineering
        print("\n4. ğŸ› ï¸ FEATURE ENGINEERING")
        print("-" * 40)
        
        feature_engineer = EnhancedFeatureEngineer(config.get('feature_params', {}))
        features = feature_engineer.create_enhanced_features(interactions)
        
        print(f"ğŸ“Š Enhanced features shape: {features.shape}")
        print(f"ğŸ“‹ New columns: {[col for col in features.columns if col not in interactions.columns]}")
        
        # 5. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ¾Ğ²
        print("\n5. ğŸ¯ CREATING TARGETS")
        print("-" * 40)
        
        targets = feature_engineer.create_improved_targets(features)
        
        if targets is None:
            print("âŒ Failed to create targets")
            return None, None
        
        # 6. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        print("\n6. ğŸ’¾ SAVING RESULTS")
        print("-" * 40)
        
        root_dir = Path(__file__).parent.parent.parent
        processed_dir = root_dir / 'data' / 'processed'
        processed_dir.mkdir(parents=True, exist_ok=True)
        
        features_path = processed_dir / 'features.parquet'
        targets_path = processed_dir / 'targets.parquet'
        
        features.to_parquet(features_path)
        pd.DataFrame({'target': targets}).to_parquet(targets_path)
        
        print(f"âœ… Features saved to: {features_path}")
        print(f"âœ… Targets saved to: {targets_path}")
        print(f"ğŸ“Š Final features shape: {features.shape}")
        print(f"ğŸ¯ Target distribution: {targets.sum()}/{len(targets)} positive ({targets.mean():.2%})")
        
        print("\n" + "=" * 60)
        print("âœ… FEATURE ENGINEERING COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        
        return features, targets
        
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()
